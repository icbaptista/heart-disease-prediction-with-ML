{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461da679",
   "metadata": {},
   "source": [
    "**Imports and data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd080dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 1: age in years\n",
    "# Column 2: 1 = male, 0 = female\n",
    "# Column 3: 1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic\n",
    "# Column 4: resting blood pressure in mmHg\n",
    "# Column 5: cholesterol in mg/dl\n",
    "# Column 6: 1 means > 120 mg/dl, 0 means <= 120 mg/dl\n",
    "# Column 7: resting electrocardiographic, 0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "# Column 8: max heart rate achieved\n",
    "# Column 9: 1 = yes, 2 = no\n",
    "# Column 10: ST depression induced by exercise relative to rest\n",
    "# Column 11: slope of the peak exercise ST segment, 1 = upsloping, 2 = flat, 3 = downsloping\n",
    "# Column 12: number of major vessels (0-3) colored by flourosopy\n",
    "# Column 13: 3 = normal, 6 = fixed defect, 7 = reversable defect\n",
    "# Column 14: >=1 means heart disease, 0 = no heart disease\n",
    "\n",
    "myNames = [\"age\", # column 1\n",
    "        \"sex\", # col 2\n",
    "        \"chestPainType\", # col 3\n",
    "        \"restingBP\", # col 4\n",
    "        \"serumCholesterol\", # col 5\n",
    "        \"fastingBloodSugar\", # col 6\n",
    "        \"restingEcg\", # col 7\n",
    "        \"maxHeartRate\", # col 8\n",
    "        \"exerciseInducedAngina\", # col 9\n",
    "        \"stDepression\", # col 10\n",
    "        \"stSlope\", # col 11\n",
    "        \"majorVessels\", # col 12\n",
    "        \"thalassemia\", # col 13\n",
    "        \"diagnosis\"] # col 14\n",
    "assert len(myNames) == 14\n",
    "\n",
    "data=pd.read_csv(\"data/processed.cleveland.data\", sep=',', names=myNames, na_values=[\"?\", '?'])\n",
    "data = data.dropna() # Drop rows with missing data\n",
    "data.iloc[:, -1] = data.iloc[:, -1].clip(0, 1) # clip last column (output) between 0 and 1, both inclusive\n",
    "\n",
    "# Convert all columns to float\n",
    "for name in myNames:\n",
    "    data[name] = pd.to_numeric(data[name],errors = 'coerce')\n",
    "\n",
    "m = data.shape[0] # num examples\n",
    "n = data.shape[1] - 1 # num features\n",
    "\n",
    "print(\"Num features:\", n)\n",
    "print(\"Num examples:\", m)\n",
    "    \n",
    "# a few examples from the dataset \n",
    "print()\n",
    "print(data.head())\n",
    "\n",
    "X = data.values[:, :-1]\n",
    "y = data.values[:, -1:]\n",
    "\n",
    "\n",
    "\n",
    "categoricalColumnsNames = [\"sex\", \n",
    "                      \"chestPainType\", \n",
    "                      \"fastingBloodSugar\", \n",
    "                      \"restingEcg\", \n",
    "                      \"exerciseInducedAngina\", \n",
    "                      \"stSlope\",\n",
    "                      \"majorVessels\",\n",
    "                      \"thalassemia\"]\n",
    "continuousColumns = data.drop(categoricalColumnsNames, axis=1)\n",
    "continuousColumnsNames = [name for name in myNames if name not in categoricalColumnsNames and name != \"diagnosis\"]\n",
    "categoricalColumns = data.drop(continuousColumnsNames, axis=1)\n",
    "if \"diagnosis\" in continuousColumns:\n",
    "    continuousColumns = continuousColumns.drop(\"diagnosis\", axis=1)\n",
    "if \"diagnosis\" not in categoricalColumns:\n",
    "    categoricalColumns = data[\"diagnosis\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24930cc",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef07643b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9985/1906467310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"diagnosis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"diagnosis\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(\"diagnosis\", axis=1), data[\"diagnosis\"], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "def trainUnregul():\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Log Reg Unregularized\")\n",
    "    print(\"No penalty\")\n",
    "    print(\"Accuracy (% correct predicts)\", round(accuracy, 2))\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(\"Balanced accuracy:\", round(balanced_accuracy,2))\n",
    "    print(\"Precision:\", round(precision, 2))\n",
    "    print(\"Recall:\", round(recall, 2))\n",
    "    print(\"F1-score:\", round(f1, 2))\n",
    "    \n",
    "def trainRegul(argPenalty=\"l2\", argC=[0.01, 0.05, 0.1, 0.3, 0.5,  0.8, 1, 2, 5]):\n",
    "    bestAcc = 0\n",
    "    bestC = None\n",
    "    for myC in argC:\n",
    "        model = LogisticRegression(max_iter=10000, penalty=argPenalty, C=myC)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if accuracy > bestAcc:\n",
    "            bestAcc = accuracy\n",
    "            bestC = myC\n",
    "    print(\"Log Reg Regularized\")\n",
    "    print(\"Penalty \" + argPenalty)\n",
    "    print(\"Best C\", bestC)\n",
    "    print(\"Accuracy (% correct predicts)\", round(bestAcc,2))\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(\"Balanced accuracy:\", round(balanced_accuracy,2))\n",
    "    print(\"Precision:\", round(precision, 2))\n",
    "    print(\"Recall:\", round(recall, 2))\n",
    "    print(\"F1-score:\", round(f1, 2))\n",
    "    \n",
    "train()\n",
    "print()\n",
    "trainRegul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a89247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \\\n",
    "    classification_report\n",
    "\n",
    "# Encode the categorical variables\n",
    "# It transforms each unique value of the categorical variable into a separate binary variable.\n",
    "# Multiple columns of True/False values\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data_encoded.drop('diagnosis', axis=1)\n",
    "y = data_encoded['diagnosis']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "# Fit a logistic regression model to the training set\n",
    "# increase the maximum number of iterations and specify the solver\n",
    "model = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the model\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y = data['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "# Create decision tree classifier with default settings\n",
    "dt = DecisionTreeClassifier(random_state=60)\n",
    "\n",
    "# Fit the decision tree on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Convert categorical variables to dummies\n",
    "data_encoded = pd.get_dummies(data, columns=[\"chestPainType\", \"restingEcg\", \"stSlope\", \"thalassemia\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data_encoded.drop('diagnosis', axis=1)\n",
    "y = data_encoded['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=60)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \\\n",
    "    classification_report\n",
    "\n",
    "# Encode the categorical variables\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data_encoded.drop('diagnosis', axis=1)\n",
    "y = data_encoded['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "# Fit a GBM model to the training set\n",
    "model = GradientBoostingClassifier(random_state=60, learning_rate=0.1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the model\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(\"diagnosis\", axis=1)\n",
    "y = data[\"diagnosis\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=16, activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the neural network\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6633775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
